# Ultralytics YOLO ðŸš€, AGPL-3.0 license
Model settings:
  task:
    value: detect
    type: str
    desc: YOLO task, i.e. detect, segment, classify, pose, obb
  mode:
    value: train
    type: str
    desc: YOLO mode, i.e. train, val, predict, export, track, benchmark
  cfg:
    value: null
    type: str
    desc: used to override settings in default.yaml file.

Train settings:
  model:
    value: null
    type: str
    desc: >
      Specifies the model file for training. Accepts a path to either a `.pt`
      pretrained model or a `.yaml` configuration file. Essential for
      defining the model structure or initializing weights.
  data: &data
    value: null
    type: str
    desc: >
      Path to the dataset configuration file (e.g., `coco128.yaml`). This file
      contains dataset-specific parameters, including paths to training and
      validation data, class names, and number of classes.
  epochs:
    value: 100
    type: int
    desc: >
      Total number of training epochs. Each epoch represents a full pass over
      the entire dataset. Adjusting this value can affect training duration and
      model performance.
  time:
    value: null
    type: int
    desc: >
      Maximum training time in hours. If set, this overrides the `epochs`
      argument, allowing training to automatically stop after the 
      specified duration. Useful for time-constrained training scenarios.
  patience:
    value: 100
    type: int
    desc: >
      Number of epochs to wait without improvement in validation metrics
      before early stopping the training. Helps prevent overfitting by 
      stopping training when performance plateaus.
  batch: &batch
    value: 16
    type: int
    desc: >
      Batch size for training, indicating how many images are processed before
      the model's internal parameters are updated. AutoBatch (`batch=-1`) 
      dynamically adjusts the batch size based on GPU memory availability.
  imgsz: &imgsz
    value: 640
    type: int | list
    desc: >
      Target image size for training. All images are resized to this dimension
      before being fed into the model. Affects model accuracy and computational
      complexity.
  save:
    value: True
    type: bool
    desc: |
      Enables saving of training checkpoints and final model weights. Useful
      for resuming training or model deployment.
  save_period:
    value: -1
    type: int
    desc: >
      Frequency of saving model checkpoints, specified in epochs. A value of
      -1 disables this feature. Useful for saving interim models during long
      training sessions.
  cache:
    value: False
    type: bool | str
    desc: >
      Enables caching of dataset images in memory (`True`/`ram`), on disk
      (`disk`), or disables it (`False`). Improves training speed by reducing
      disk I/O at the cost of increased memory usage.
  device: &device
    value: null
    type: int | str | list
    desc: >
      "Specifies the computational device(s) for training: a single GPU
      (`device=0`), multiple GPUs (`device=0,1`), CPU (`device=cpu`), or MPS
      for Apple silicon (`device=mps`)."
  workers:
    value: 8
    type: int
    desc: >
      Number of worker threads for data loading (per `RANK` if Multi-GPU
      training). Influences the speed of data preprocessing and feeding into
      the model, especially useful in multi-GPU setups.
  project:
    value: null
    type: str
    desc: >
      Name of the project directory where training outputs are saved. Allows
      for organized storage of different experiments.
  name:
    value: null
    type: str
    desc: >
      Name of the training run. Used for creating a subdirectory within the
      project folder, where training logs and outputs are stored.
  exist_ok:
    value: False
    type: bool
    desc: >
      If True, allows overwriting of an existing project/name directory. Useful
      for iterative experimentation without needing to manually clear previous
      outputs.
  pretrained:
    value: True
    type: bool
    desc: >
      Determines whether to start training from a pretrained model. Can be a
      boolean value or a string path to a specific model from which to load
      weights. Enhances training efficiency and model performance.
  optimizer:
    value: auto
    type: str
    desc: >
      Choice of optimizer for training. Options include `SGD`, `Adam`, `AdamW`,
      `NAdam`, `RAdam`, `RMSProp` etc., or `auto` for automatic selection based
      on model configuration. Affects convergence speed and stability.
  verbose:
    value: True
    type: bool
    desc: >
      Enables verbose output during training, providing detailed logs and
      progress updates. Useful for debugging and closely monitoring the
      training process.
  seed:
    value: 0
    type: int
    desc: >
      Sets the random seed for training, ensuring reproducibility of results
      across runs with the same configurations.
  deterministic:
    value: True
    type: bool
    desc: >
      Forces deterministic algorithm use, ensuring reproducibility but may
      affect performance and speed due to the restriction on non-deterministic
      algorithms.
  single_cls:
    value: False
    type: bool
    desc: >
      Treats all classes in multi-class datasets as a single class during 
      training. Useful for binary classification tasks or when focusing on
      object presence rather than classification.
  rect: &rect
    value: False
    type: bool
    desc: >
      Enables rectangular training, optimizing batch composition for minimal
      padding. Can improve efficiency and speed but may affect model accuracy.
  cos_lr:
    value: False
    type: bool
    desc: >
      Utilizes a cosine learning rate scheduler, adjusting the learning rate
      following a cosine curve over epochs. Helps in managing learning rate
      for better convergence.
  close_mosaic:
    value: 10
    type: int
    desc: >
      Disables mosaic data augmentation in the last N epochs to stabilize
      training before completion. Setting to 0 disables this feature.
  resume:
    value: False
    type: bool
    desc: >
      Resumes training from the last saved checkpoint. Automatically loads
      model weights, optimizer state, and epoch count, continuing training
      seamlessly.
  amp:
    value: True
    type: bool
    desc: >
      Enables Automatic Mixed Precision (AMP) training, reducing memory usage
      and possibly speeding up training with minimal impact on accuracy.
  fraction:
    value: 1.0
    type: float
    desc: >
      Specifies the fraction of the dataset to use for training. Allows for
      training on a subset of the full dataset, useful for experiments or when
      resources are limited.
  profile:
    value: False
    type: bool
    desc: >
      Enables profiling of ONNX and TensorRT speeds during training, useful for
      optimizing model deployment.
  freeze:
    value: null
    type: int | list
    desc: >
      Freezes the first N layers of the model or specified layers by index,
      reducing the number of trainable parameters. Useful for fine-tuning or
      transfer learning.
  val:
    value: True
    type: bool
    desc: >
      Enables validation during training, allowing for periodic evaluation of
      model performance on a separate dataset.
  plots: &plots
    value: True
    type: bool
    desc: >
      Generates and saves plots of training and validation metrics, as well as
      prediction examples, providing visual insights into model performance and
      learning progression.

  Segmentation train settings:
    overlap_mask:
      value: True
      type: bool
      desc: >
        Determines whether segmentation masks should overlap during training,
        applicable in instance segmentation tasks.
    mask_ratio:
      value: 4
      type: int
      desc: >
        Downsample ratio for segmentation masks, affecting the resolution of
        masks used during training.

  Classification train settings:
    dropout:
      value: 0.0
    type: float
    desc: >
      Dropout rate for regularization in classification tasks, preventing 
      overfitting by randomly omitting units during training.

Hyperparameters:
  lr0:
    value: null
    type:
    desc: >
      Initial learning rate (i.e. `SGD=1E-2`, `Adam=1E-3`) . Adjusting this
      value is crucial for the optimization process, influencing how rapidly
      model weights are updated.
  lrf:
    value: null
    type:
    desc: >
      Final learning rate as a fraction of the initial rate = (`lr0 * lrf`),
      used in conjunction with schedulers to adjust the learning rate over
      time.
  momentum:
    value: null
    type:
    desc: >
      Momentum factor for SGD or beta1 for Adam optimizers, influencing the
      incorporation of past gradients in the current update.
  weight_decay:
    value: null
    type:
    desc: >
      L2 regularization term, penalizing large weights to prevent overfitting.
  warmup_epochs:
    value: null
    type:
    desc: >
      Number of epochs for learning rate warmup, gradually increasing the
      learning rate from a low value to the initial learning rate to stabilize
      training early on.
  warmup_momentum:
    value: null
    type:
    desc: >
      Initial momentum for warmup phase, gradually adjusting to the set
      momentum over the warmup period.
  warmup_bias_lr:
    value: null
    type:
    desc: >
      Learning rate for bias parameters during the warmup phase, helping
      stabilize model training in the initial epochs.
  box:
    value: null
    type:
    desc: >
      Weight of the box loss component in the loss function, influencing how
      much emphasis is placed on accurately predicting bounding box
      coordinates.
  cls:
    value: null
    type:
    desc: >
      Weight of the classification loss in the total loss function, affecting
      the importance of correct class prediction relative to other components.
  dfl:
    value: null
    type:
    desc: >
      Weight of the distribution focal loss, used in certain YOLO versions for
      fine-grained classification.
  pose:
    value: null
    type:
    desc: >
      Weight of the pose loss in models trained for pose estimation,
      influencing the emphasis on accurately predicting pose keypoints.
  kobj:
    value: null
    type:
    desc: >
      Weight of the keypoint objectness loss in pose estimation models,
      balancing detection confidence with pose accuracy.
  label_smoothing:
    value: null
    type:
    desc: >
      Applies label smoothing, softening hard labels to a mix of the target
      label and a uniform distribution over labels, can improve generalization.
  nbs:
    value: null
    type:
    desc: Nominal batch size for normalization of loss.
  
  Augmentation Hyperparameters:
    hsv_h:
      value: null
      type:
      desc: >
        Adjusts the hue of the image by a fraction of the color wheel,
        introducing color variability. Helps the model generalize across
        different lighting conditions.
    hsv_s:
      value: null
      type:
      desc: >
        Alters the saturation of the image by a fraction, affecting the 
        intensity of colors. Useful for simulating different environmental.
    hsv_v:
      value: null
      type:
      desc: >
        Modifies the value (brightness) of the image by a fraction, helping
        the model to perform well under various lighting conditions.
    degrees:
      value: null
      type:
      desc: >
        Rotates the image randomly within the specified degree range, improving
        the model's ability to recognize objects at various orientations.
    translate:
      value: null
      type:
      desc: >
        Translates the image horizontally and vertically by a fraction of the
        image size, aiding in learning to detect partially  visible objects.
    scale:
      value: null
      type:
      desc: > 
      Scales the image by a gain factor, simulating objects at different
      distances from the camera.
    shear:
      value: null
      type:
      desc: >
        Shears the image by a specified degree, mimicking the effect of objects
        being viewed from different angles.
    perspective:
      value: null
      type:
      desc: >
        Applies a random perspective transformation to the image, enhancing the
        model's ability to understand objects in 3D space.
    flipud:
      value: null
      type:
      desc: >
        Flips the image upside down with the specified probability, increasing
        the data variability without affecting the object's characteristics.
    fliplr:
      value: null
      type:
      desc: >
        Flips the image left to right with the specified probability, useful
        for learning symmetrical objects and increasing dataset diversity.
    mosaic:
      value: null
      type:
      desc: >
        Combines four training images into one, simulating different scene
        compositions and object interactions. Highly effective for complex
        scene understanding.
    mixup:
      value: null
      type:
      desc: >
        Blends two images and their labels, creating a composite image.
        Enhances the model's ability to generalize by introducing label noise
        and visual variability.

    Segmentation Augmentations:
      copy_paste:
        value: null
      type:
      desc: >
        Copies objects from one image and pastes them onto another, useful for
        increasing object instances and learning object occlusion.

    Classification Augmentations:
      auto_augment:
        value: null
      type:
      desc: >
        Automatically applies a predefined augmentation policy (`randaugment`,
        `autoaugment`, `augmix`), optimizing for classification tasks by
        diversifying the visual features.
      erasing:
        value: null
      type:
      desc: >
        Randomly erases a portion of the image during classification training,
        encouraging the model to focus on less obvious features for recognition.
      crop_fraction:
        value: null
      type:
      desc: Fractional amount to crop image height and width

Val/Test settings:
  data: *data
    # value: null
    # type:
    # desc: >
      # Specifies the path to the dataset configuration file (e.g.,
      # `coco128.yaml`). This file includes paths to validation data, class
      # names, and number of classes.
  imgsz: *imgsz
    # value: 640
    # type: int | list
    desc: >
      # Defines the size of input images. All images are resized to this
      # dimension before processing.
  batch: *batch
    # value: 16
    # type: int
    # desc: >
      # Sets the number of images per batch. Use `-1` for AutoBatch, which
      # automatically adjusts based on GPU memory availability.
  save_json:
    value: null
    type:
    desc: >
      If `True`, saves the results to a JSON file for further analysis or
      integration with other tools.
  save_hybrid:
    value: null
    type:
    desc: >
      If `True`, saves a hybrid version of labels that combines original
      annotations with additional model predictions.
  conf:
    value: 0.001
    type: float
    desc: >
      Sets the minimum confidence threshold for detections. Detections with
      confidence below this threshold are discarded. Adjusting this value can
      help reduce false positives.
  iou: &iou
    value: 0.7
    type: float
    desc: >
      Intersection Over Union (IoU) threshold for Non-Maximum Suppression
      (NMS). Higher values result in fewer detections by eliminating overlapping
      boxes, useful for reducing duplicates.
  max_det: &max_det
    value: 300
    type: int
    desc: >
      Maximum number of detections allowed per image. Limits the total number
      of objects the model can detect in a single inference, preventing
      excessive outputs in dense scenes.
  half: &half
    value: False
    type: bool
    desc: >
      Enables half-precision (FP16) inference, which can speed up model
      inference on supported GPUs with minimal impact on accuracy.
  device: *device
    # value: null
    # type: int | str | list
    # desc: >
      # Specifies the device for validation (`cpu`, `cuda:0`, etc.). Allows 
      # flexibility in utilizing CPU or GPU resources.
  dnn:
    value: False
    type: bool
    desc: >
      If `True`, uses OpenCV's DNN module for ONNX model inference, offering an
      alternative to PyTorch inference methods.
  plots: *plots
    # value: True
    # type: bool
    # desc: >
      # Generates and saves plots of training and validation metrics, as well as
      # prediction examples, providing visual insights into model performance
      # and learning progression.
  rect: *rect
    # value: False
    # type: bool
    # desc: >
      # Enables rectangular training, optimizing batch composition for minimal
      # padding. Can improve efficiency and speed but may affect model accuracy.
  split:
    value: val
    type: str
    desc: >
      Determines the dataset split to use for validation (`val`, `test`, or
      `train`). Allows flexibility in choosing the data segment for performance
      evaluation.

Predict settings:
  source:
    value: null
    type: str
    desc: >
      Specifies the data source for inference. Can be an image path, video
      file, directory, URL, or device ID for live feeds. Supports a wide range
      of formats and sources, enabling flexible application across different
      types of input.
  conf:
    value: 0.25
    type: float
    desc: >
      Sets the minimum confidence threshold for detections. Objects detected
      with confidence below this threshold will be disregarded. Adjusting this
      value can help reduce false positives.
  iou: *iou
    # value: 0.7
    # type: float
    # desc: >
      # Intersection Over Union (IoU) threshold for Non-Maximum Suppression
      # (NMS). Higher values result in fewer detections by eliminating overlapping
      # boxes, useful for reducing duplicates.
  imgsz:
    value: 640
    type: int | tuple
    desc: >
      Defines the image size for inference. Can be a single integer `640` for
      square resizing or a (height, width) tuple. Proper sizing can improve
      detection accuracy and processing speed.
  half: *half
    # value: False
    # type: bool
    # desc: >
      # Enables half-precision (FP16) inference, which can speed up model
      # inference on supported GPUs with minimal impact on accuracy.
  device: *device
    # value: null
    # type: int | str | list
    # desc: >
      # Specifies the device for inference (e.g., `cpu`, `cuda:0` or `0`). Allows
      # users to select between CPU, a specific GPU, or other compute devices for
      # model execution.
  max_det: *max_det
    # value: 300
    # type: int
    # desc: >
      # Maximum number of detections allowed per image. Limits the total number
      # of objects the model can detect in a single inference, preventing
      # excessive outputs in dense scenes.
  vid_stride:
    value: 1
    type: int
    desc: >
      Frame stride for video inputs. Allows skipping frames in videos to speed
      up processing at the cost of temporal resolution. A value of 1 processes
      every frame, higher values skip frames.
  stream_buffer:
    value: False
    type: bool
    desc: >
      Determines if all frames should be buffered when processing video streams
      (`True`), or if the model should return the most recent frame (`False`).
      Useful for real-time applications.
  visualize:
    value: False
    type: bool
    desc: >
      Activates visualization of model features during inference, providing
      insights into what the model is "seeing". Useful for debugging and model
      interpretation.
  augment:
    value: False
    type: bool
    desc: >
      Enables test-time augmentation (TTA) for predictions, potentially
      improving detection robustness at the cost of inference speed.
  agnostic_nms:
    value: False
    type: bool
    desc: >
      Enables class-agnostic Non-Maximum Suppression (NMS), which merges
      overlapping boxes of different classes. Useful in multi-class detection
      scenarios where class overlap is common.
  classes:
    value: null
    type: int | list[int]
    desc: >
      Filters predictions to a set of class IDs. Only detections belonging to
      the specified classes will be returned. Useful for focusing on relevant
      objects in multi-class detection tasks.
  retina_masks:
    value: False
    type: bool
    desc: >
      Uses high-resolution segmentation masks if available in the model. This
      can enhance mask quality for segmentation tasks, providing finer detail.
  embed:
    value: null
    type: list[int]
    desc: >
      Specifies the layers from which to extract feature vectors or embeddings.
      Useful for downstream tasks like clustering or similarity search.

  Visualize settings:
    show:
      value: False
      type: bool
      desc: >
        If `True`, displays the annotated images or videos in a window. Useful
        for immediate visual feedback during development or testing.
    save:
      value: False
      type: bool
      desc: >
        Enables saving of the annotated images or videos to file. Useful for
        documentation, further analysis, or sharing results.
    save_frames:
      value: False
      type: bool
      desc: >
        When processing videos, saves individual frames as images. Useful for
        extracting specific frames or for detailed frame-by-frame analysis.
    save_txt:
      value: False
      type: bool
      desc: >
        Saves detection results in a text file, following the format
        `[class] [x_center] [y_center] [width] [height] [confidence]`.
        Useful for integration with other analysis tools.
    save_conf:
      value: False
      type: bool
      desc: >
        Includes confidence scores in the saved text files. Enhances the detail
        available for post-processing and analysis.
    save_crop:
      value: False
      type: bool
      desc: >
        Saves cropped images of detections. Useful for dataset augmentation,
        analysis, or creating focused datasets for specific objects.
    show_labels:
      value: True
      type: bool
      desc: >
        Displays labels for each detection in the visual output. Provides
        immediate understanding of detected objects.
    show_conf:
      value: True
      type: bool
      desc: >
        Displays the confidence score for each detection alongside the label.
        Gives insight into the model's certainty for each detection.
    show_boxes:
      value: True
      type: bool
      desc: >
        Draws bounding boxes around detected objects. Essential for visual 
        identification and location of objects in images or video frames.
    line_width:
      value: null
      type: int
      desc: >
        Specifies the line width of bounding boxes. If `None`, the line width
        is automatically adjusted based on the image size. Provides visual
        customization for clarity.

  Tracker settings:
    tracker:
      value: botsort.yaml
      type: str
      desc: tracker type, choices=[botsort.yaml, bytetrack.yaml]

Export settings:
  format:
    value: torchscript
    type: str
    desc: >
      Target format for the exported model, such as `'onnx'`, `'torchscript'`,
      `'tensorflow'`, or others, defining compatibility with various deployment
      environments.
  imgsz:
    value: 640
    type: int | tuple
    desc: >
      Desired image size for the model input. Can be an integer for square
      images or a tuple `(height, width)` for specific dimensions.
  keras:
    value: False
    type: bool
    desc: >
      Enables export to Keras format for TensorFlow SavedModel, providing
      compatibility with TensorFlow serving and APIs.
  optimize:
    value: False
    type: bool
    desc: >
      Applies optimization for mobile devices when exporting to TorchScript,
      potentially reducing model size and improving performance.
  half: *half
    # value: False
    # type: bool
    # desc: >
      # Enables FP16 (half-precision) quantization, reducing model size and
      # potentially speeding up inference on supported hardware.
  int8:
    value: False
    type: bool
    desc: >
      Activates INT8 quantization, further compressing the model and speeding
      up inference with minimal accuracy loss, primarily for edge devices.
  dynamic:
    value: False
    type: bool
    desc: >
      Allows dynamic input sizes for ONNX and TensorRT exports, enhancing
      flexibility in handling varying image dimensions.
  simplify:
    value: False
    type: bool
    desc: >
      Simplifies the model graph for ONNX exports, potentially improving
      performance and compatibility.
  opset:
    value: null
    type: int
    desc: >
      Specifies the ONNX opset version for compatibility with different ONNX
      parsers and runtimes. If not set, uses the latest supported version.
  workspace:
    value: 4
    type: int
    desc: >
      Sets the maximum workspace size in GB for TensorRT optimizations,
      balancing memory usage and performance.
  nms:
    value: False
    type: bool
    desc: >
      Adds Non-Maximum Suppression (NMS) to the CoreML export, essential for
      accurate and efficient detection post-processing.
